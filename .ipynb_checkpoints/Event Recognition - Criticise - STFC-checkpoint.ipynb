{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import configparser\n",
    "import pathlib as p\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clipper_python as clipper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from frag_nn.pytorch.network import GNINA_regressor, GNINA_regressor_v2, GNINA_regressor_v3, GNINA_regressor_v4, GNINA_regressor_v5, GNINA_regressor_v6, GNINA_regressor_v7, GNINA_regressor_v8\n",
    "# from frag_nn.data import XChemData\n",
    "from frag_nn.pytorch.network import ClassifierV3, ClassifierV4, ClassifierV5\n",
    "from frag_nn.pytorch.dataset import EventDataset\n",
    "from frag_nn.pytorch.dataset import OrthogonalGrid\n",
    "from frag_nn.pytorch.dataset import GetRandomisedLocation, GetRandomisedRotation, SetRoot\n",
    "from frag_nn.pytorch.dataset import GetAnnotationClassifier, GetDataRefMoveZ\n",
    "\n",
    "from frag_nn.pytorch.dataset import XChemDataset\n",
    "import frag_nn.constants as c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"/home/zoh22914/pandda_nn_2/frag_nn/params.ini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = configparser.ConfigParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.read(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_conf = conf[c.x_chem_database]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 48\n",
    "grid_step = 0.5\n",
    "filters = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_type = \"classifier\"\n",
    "network_version = 5\n",
    "dataset_version = 3\n",
    "train = \"gpu\"\n",
    "transforms = \"rottrans\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict_dir = \"/home/zoh22914/pandda_nn_2/\"\n",
    "state_dict_file = state_dict_dir + \"model_params_{}_{}_{}_{}_{}_{}_{}_{}.pt\".format(grid_size,\n",
    "                                                                                  grid_step,\n",
    "                                                                                  network_type,\n",
    "                                                                                  network_version,\n",
    "                                                                                  dataset_version,\n",
    "                                                                                  train,\n",
    "                                                                                  transforms,\n",
    "                                                                                     filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get accessible events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_test = pd.read_csv(\"new_events_test_no_cheat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = OrthogonalGrid(grid_size, \n",
    "                     grid_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = EventDataset(events=events_test,\n",
    "                             transforms_record=[GetRandomisedLocation(base_trans_max=4.0, secondary_trans_max=0.0),\n",
    "                                                    GetRandomisedRotation(max_rot=0.0),\n",
    "                                                    SetRoot(\"/data/data\")\n",
    "                                               ],\n",
    "                             get_annotation=GetAnnotationClassifier(),\n",
    "                             get_data=GetDataRefMoveZ(grid)\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                         batch_size=1, \n",
    "                                         shuffle=True,\n",
    "                                         num_workers=48)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassifierV5(filters,\n",
    "                        grid_dimension=grid_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(state_dict_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c = model.to(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision - Recall functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision(y_hat, y, cutoff):\n",
    "    \n",
    "    positives_hat_mask = (y_hat > cutoff)\n",
    "    negatives_hat_mask = (y_hat <= cutoff)\n",
    "    \n",
    "    positives_mask = (y == 1)\n",
    "    negatives_mask = (y == 0)\n",
    "\n",
    "    true_positives = np.count_nonzero(positives_hat_mask[positives_mask])\n",
    "    false_positives = np.count_nonzero(positives_hat_mask[negatives_mask])\n",
    "    \n",
    "    total_predicted_positives = true_positives + false_positives\n",
    "    \n",
    "    if total_predicted_positives == 0:\n",
    "        return 1\n",
    "    \n",
    "    precision = true_positives / total_predicted_positives\n",
    "    \n",
    "    return precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recall(y_hat, y, cutoff):\n",
    "    positives_hat_mask = (y_hat > cutoff)\n",
    "    negatives_hat_mask = (y_hat <= cutoff)\n",
    "\n",
    "    positives_mask = (y == 1)\n",
    "    negatives_mask = (y == 0)\n",
    "\n",
    "    true_positives = np.count_nonzero(positives_hat_mask[positives_mask])\n",
    "    false_negatives = np.count_nonzero(negatives_hat_mask[positives_mask])    \n",
    "\n",
    "    total_positives = (true_positives + false_negatives)\n",
    "    \n",
    "    if total_positives == 0:\n",
    "        return 0\n",
    "    \n",
    "    recall = true_positives / total_positives\n",
    "\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate - Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = []\n",
    "y_test_hat = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, data in enumerate(test_dataloader):\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "    print(\"Iteration: {}\".format(i))\n",
    "    x = data[\"data\"]\n",
    "    y = data[\"annotation\"]\n",
    "#     x = x.unsqueeze(1)\n",
    "    y = y.view(-1,2)\n",
    "    \n",
    "    x_c = x.to(\"cuda:1\")\n",
    "    y_c = y.to(\"cuda:1\")\n",
    "    \n",
    "    outputs_c = model_c(x_c)\n",
    "    \n",
    "    outputs = outputs_c.detach().to(\"cpu\")\n",
    "    print(outputs)\n",
    "    y_test.append(y.detach())\n",
    "    y_test_hat.append(outputs)\n",
    "#     optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# float(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.cat(y_test)[:,1]\n",
    "y_hat = torch.cat(y_test_hat)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranking(df):\n",
    "    rankings = []\n",
    "    for i in range(len(df)):\n",
    "        truncated_df = df.iloc[:i] \n",
    "        recall_high = len(truncated_df[truncated_df[\"Ligand Confidence\"] == \"High\"])\n",
    "        recall_med = len(truncated_df[truncated_df[\"Ligand Confidence\"] == \"Medium\"])\n",
    "        recall= recall_high + recall_med\n",
    "        record = {\"length\": i,\n",
    "                 \"recall\": recall}\n",
    "        rankings.append(record)\n",
    "        \n",
    "    return pd.DataFrame(rankings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Append to table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_test[\"nn_score\"] = y_hat\n",
    "events_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull size sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_df = events_test[[\"Ligand Confidence\", \"cluster_size\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_size_df = size_df.sort_values(\"cluster_size\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_size_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_rankings_df = get_ranking(sorted_size_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_rankings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull NN sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_score_df = events_test[[\"Ligand Confidence\", \"nn_score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_nn_score_df = nn_score_df.sort_values(\"nn_score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_nn_score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_score_rankings_df = get_ranking(sorted_nn_score_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_score_rankings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfect_df = events_test[[\"Ligand Confidence\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfect_df[\"score\"] = events_test[\"Ligand Confidence\"]\n",
    "perfect_df[\"score\"][perfect_df[\"score\"] == \"High\"] = 0\n",
    "perfect_df[\"score\"][perfect_df[\"score\"] == \"Medium\"] = 1\n",
    "perfect_df[\"score\"][perfect_df[\"score\"] == \"Low\"] = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfect_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_perfect_df = perfect_df.sort_values(\"score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfect_rankings_df = get_ranking(sorted_perfect_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perfect_rankings_df[\"recall\"] = list(range(len(perfect_rankings_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfect_rankings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_df = events_test[[\"Ligand Confidence\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_random_df = random_df.sample(len(random_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_rankings_df = get_ranking(sorted_random_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall_length_df = pd.DataFrame({\"length\": nn_score_rankings_df[\"length\"],\n",
    "#                                 \"recall_nn_score\": nn_score_rankings_df[\"recall\"],\n",
    "#                                 \"recall_size_score\": size_rankings_df[\"recall\"]})\n",
    "recall_length_df = pd.concat([nn_score_rankings_df[[\"recall\"]], \n",
    "                              size_rankings_df[[\"recall\"]],\n",
    "                             random_rankings_df[[\"recall\"]],\n",
    "                             perfect_rankings_df[[\"recall\"]]], \n",
    "                             keys=[\"nn_score\", \"cluster_size\", \"random\", \"perfect\"], \n",
    "                             names=[\"score\", \"len\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_length_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_length_df = recall_length_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_length_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall_length_df.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall_length_df.stack().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=\"length\",\n",
    "               y=\"recall\",\n",
    "               data=size_rankings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=\"length\",\n",
    "               y=\"recall\",\n",
    "               data=nn_score_rankings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, ax = plt.subplots(figsize=(20,20))\n",
    "\n",
    "sns.lineplot(x=\"len\",\n",
    "               y=\"recall\",\n",
    "               data=recall_length_df,\n",
    "             hue=\"score\",\n",
    "            ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get recall and precission for different cutoffs\n",
    "points = []\n",
    "for cutoff in np.linspace(0, 1, 100):\n",
    "    precision = get_precision(y_hat, y, cutoff)\n",
    "    recall = get_recall(y_hat, y, cutoff)\n",
    "    points.append({\"cutoff\": cutoff,\n",
    "                   \"precision\": precision, \n",
    "                   \"recall\":recall,\n",
    "                  \"num_predicted_positives\": len(y_hat[y_hat > cutoff]),\n",
    "                  \"num_true_positives\": len(y[y_hat > cutoff][y[y_hat > cutoff] == 1])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = pd.DataFrame(points).set_index(\"cutoff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=\"recall\",\n",
    "               y=\"precision\",\n",
    "               data=stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"recall\",\n",
    "               y=\"precision\",\n",
    "               data=stats,\n",
    "            estimator=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.iloc[0].num_true_positives / stats.iloc[0].num_predicted_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_precision = len(events_test[events_test[\"Ligand Confidence\"] == \"High\"]) / len(events_train)\n",
    "base_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate - Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = []\n",
    "y_test_hat = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(train_dataloader):\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "    print(\"Iteration: {}\".format(i))\n",
    "    x = data[\"x\"]\n",
    "    y = data[\"y\"]\n",
    "#     x = x.unsqueeze(1)\n",
    "    y = y.view(-1,1)\n",
    "    \n",
    "    outputs = model(x)\n",
    "    \n",
    "    y_test.append(y.detach())\n",
    "    y_test_hat.append(outputs.detach())\n",
    "#     optimizer.zero_grad()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.cat(y_test)\n",
    "y_hat = torch.cat(y_test_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get recall and precission for different cutoffs\n",
    "points = []\n",
    "for cutoff in np.linspace(0, 1, 50):\n",
    "    precision = get_precision(y_hat, y, cutoff)\n",
    "    recall = get_recall(y_hat, y, cutoff)\n",
    "    points.append({\"cutoff\": cutoff,\n",
    "                   \"precision\": precision, \n",
    "                   \"recall\":recall})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = pd.DataFrame(points).set_index(\"cutoff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=\"recall\",\n",
    "               y=\"precision\",\n",
    "               data=stats,\n",
    "            estimator=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"recall\",\n",
    "               y=\"precision\",\n",
    "               data=stats,\n",
    "            estimator=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_precision = len(events_train[events_train[\"ligand_confidence_inspect\"] == \"High\"]) / len(events_train)\n",
    "base_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_params.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = dataset.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = iterator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"model_32.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = create_supervised_trainer(model, optimizer, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def print"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
